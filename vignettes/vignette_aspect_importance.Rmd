---
title: "Vignette for aspect importance"
author: "Katarzyna PÄ™kala"
date: "`r Sys.Date()`"
output: rmarkdown::html_document
vignette: >
  %\VignetteIndexEntry{Survival on the RMS Titanic}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = FALSE,
  comment = "#>",
  warning = FALSE,
  message = FALSE
)
```

# Data and logistic regression model for Titanic survival

Vignette will present the aspect_importance() function on the datasets *titanic* (available in the `DALEX` package) and *BostonHousing2* (from `mlbench` package).  
At the beginning, we download *titanic* dataset and build logistic regression model.

```{r read titanic data}
library("DALEX")
head(titanic)
titanic <- na.omit(titanic)
model_titanic_glm <- glm(survived == "yes" ~ class+gender+age+sibsp+parch+fare+embarked,
               titanic, family = "binomial")

```

#  Preparing additional parameters

Before using aspect_importance() we need to:

* group features of the dataset into aspects, 
* define the size of the sample that will allow us to calculate aspect importance,
* choose observation for which we explain aspects' importance.


```{r build parameters for titanic model}
aspects <- list(wealth = c("class", "fare"), family = c("sibsp", "parch"), personal = c("age","gender"), embarked = "embarked")

B <- 100
passenger <- data.frame(
  class = factor("1st", levels = c("1st", "2nd", "3rd", "deck crew", "engineering crew", "restaurant staff", "victualling crew")),
  gender = factor("male", levels = c("female", "male")),
  age = 8,
  sibsp = 0,
  parch = 0,
  fare = 72,
  embarked = factor("Southampton", levels = c("Belfast", "Cherbourg", "Queenstown", "Southampton"))
)
passenger
predict(model_titanic_glm, passenger)

```

# Calculating aspect importance (logistic regression)

We call aspect_importance() function and see that features that are included in **wealth** (that is *class* and *fare*) have the biggest postive contribution on survival prediction for the passenger. Port of **embarkment** (only one feature in this group) has much smaller influence and is of a negative type. **Personal** (*gender*, *age*) features as well as **family** features (*sibsp*, *parch*) have sligthly positive influence. 


```{r Calculating aspect importance (logistic regression)}
library("ggplot2")
library("ingredients")

titanic_glm_ai <- aspect_importance(model_titanic_glm, titanic, predict, passenger, aspects, 100)

titanic_glm_ai
plot(titanic_glm_ai) + ggtitle("Aspect importance for the selected passenger (logistic reg.)")
```


# Calculating aspect importance with explainer

Aspect_importance() could be also called using `DALEX` explainer as showed below.

```{r Calculating aspect importance with explainer}
explain_titanic_glm <- explain(model_titanic_glm, 
                      data = titanic[,-9],
                      y = titanic$survived == "yes", 
                      predict_function = predict,
                      label = "Logistic Regression")

titanic_glm_ai <- aspect_importance(explain_titanic_glm, passenger, aspects, 100)
titanic_glm_ai

```

We can add additional information (ie. list of features that are included in every aspect) by calling add_additional_information().

```{r adding additional info to titanic}
add_additional_information(titanic_glm_ai, explain_titanic_glm$data, aspects)
```


# Random forest model for Titanic survival

Secondly, we prepare random forest model for the *titanic* dataset.

```{r  Random forest model for Titanic survival}
library("randomForest")
model_titanic_rf <- randomForest(factor(survived) == "yes" ~ gender + age + class + embarked +
                                   fare + sibsp + parch,  data = titanic)

```

# Calculating aspect importance (random forest)

After calling aspect_importance() on the random forest model, we can observe that this time every aspect has much smaller contribution to the prediction. We can notice as well that **personal** features have a little big more influence than **wealth**.

```{r Calculating aspect importance (random forest)}
titanic_rf_ai <- aspect_importance(model_titanic_rf, titanic, predict, passenger, aspects, 100)

titanic_rf_ai
plot(titanic_rf_ai) + ggtitle("Aspect importance for the selected passenger (random for.)")
```

# Automated grouping features into aspects

On *BostonHousing2* dataset, we will test function that automatically groups features into aspects (grouping is based on the features correlation). Function only works on continuous variables.  

We are importing *BostonHousing2* from `mlbench` package and choose columns with continuous features. Then we fit linear model to the data and choose observation to be explained. Target variable is cmedv. 


```{r import BostonHousing2}

library(mlbench)
data("BostonHousing2")
data <- BostonHousing2[,-c(1:5, 10)] #excluding cont. features
head(data)
x <- BostonHousing2[,-c(1:6, 10)] #excluding cont. features and target variable
new_observation <- data[10,]
model <- lm(cmedv ~., data = data)
predict(model, new_observation)
```


We run group_variables() function with cutting off level set on 0.6. In result we get a list of variables groups (aspects) where absolute value of features' pairwise correlation is at least at 0.6.

```{r}
aspects_list <- group_variables(x, 0.6)

BostonHousing2_ai <- aspect_importance(model,data, predict_function = predict,
                            new_observation,
                            aspects_list, B = 50)
BostonHousing2_ai
```

Finally, we use add_additional_information() function, with parameter *show_cor = T*, to show how features are grouped into aspects, show minimal value of pairwise correlation in group and show whether any pair of features is negatively correlated (*neg*) or not (*pos*). 

```{r}
BostonHousing2_ai_add <- add_additional_information(BostonHousing2_ai, data, aspects_list, show_cor = T)
BostonHousing2_ai_add

```

# Using lasso in aspect_importance() function

Function aspect_importance() can calculate coefficients (that is aspects' importance) by using either linear regression or lasso regression. Using lasso we can control how many nonzero coefficients are present in final explanation. To use aspect_importance() with lasso we are providing **n_var** parameter.

We are going to use *BostonHousing2* dataset again. This time we would like to group variables into aspects with a cut off level for correlation set at 0.7.

```{r}
aspects_list <- group_variables(x, 0.7)
BostonHousing2_ai <- aspect_importance(model,data, predict_function = predict,
                            new_observation,
                            aspects_list, B = 50)
BostonHousing2_ai
```
We can see that the importance of some of those groups (1, 2 and 5) is relatively small. With the help of lasso technique, we would like to check the importance of variables' aspects, while controlling that three of them should be equal to 0. 

```{r lasso demo}
BostonHousing2_ai_lasso <- aspect_importance(model,data, predict_function = predict,
                            new_observation,
                            aspects_list, B = 50, n_var = 5)
BostonHousing2_ai_lasso_add <- add_additional_information(BostonHousing2_ai_lasso, data, aspects_list, show_cor = T)
BostonHousing2_ai_lasso_add

```


# Session info

```{r}
sessionInfo()
```

