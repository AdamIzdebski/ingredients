% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/variable_dropout.R
\name{feature_importance}
\alias{feature_importance}
\alias{feature_importance.explainer}
\alias{feature_importance.default}
\title{Model Level Variable Importance - Drop in Loss after Variable Dropout}
\usage{
feature_importance(x, ...)

\method{feature_importance}{explainer}(x,
  loss_function = loss_root_mean_square, ..., type = "raw",
  n_sample = 1000)

\method{feature_importance}{default}(x, data, y, predict_function,
  loss_function = loss_root_mean_square, ..., label = class(x)[1],
  type = "raw", n_sample = 1000)
}
\arguments{
\item{x}{a model to be explained, or an explainer created with function `DALEX::explain()`.}

\item{...}{other parameters}

\item{loss_function}{a function thet will be used to assess variable importance}

\item{type}{character, type of transformation that should be applied for dropout loss. 'raw' results raw drop lossess, 'ratio' returns \code{drop_loss/drop_loss_full_model} while 'difference' returns \code{drop_loss - drop_loss_full_model}}

\item{n_sample}{number of observations that should be sampled for calculation of variable importance. If negative then variable importance will be calculated on whole dataset (no sampling).}

\item{data}{validation dataset, will be extracted from `x` if it's an explainer}

\item{y}{true labels for `data`, will be extracted from `x` if it's an explainer}

\item{predict_function}{predict function, will be extracted from `x` if it's an explainer}

\item{label}{name of the model. By default it's extracted from the 'class' attribute of the model}
}
\value{
An object of the class 'feature_importance'.
It's a data frame with calculated average response.
}
\description{
Model Level Variable Importance - Drop in Loss after Variable Dropout
}
\examples{
 \dontrun{
library("DALEX")
library("breakDown")
library("randomForest")
HR_rf_model <- randomForest(status~., data = HR, ntree = 100)
explainer_rf  <- explain(HR_rf_model, data = HR, y = HR$status)
vd_rf <- feature_importance(explainer_rf, type = "raw",
                            loss_function = loss_cross_entropy)
head(vd_rf)
plot(vd_rf)

HR_glm_model <- glm(status == "fired"~., data = HR, family = "binomial")
explainer_glm <- explain(HR_glm_model, data = HR, y = HR$status == "fired")
vd_glm <- feature_importance(explainer_glm, type = "raw",
                        loss_function = loss_root_mean_square)
head(vd_glm)
plot(vd_glm)

library("xgboost")
model_martix_train <- model.matrix(status == "fired" ~ . -1, HR)
data_train <- xgb.DMatrix(model_martix_train, label = HR$status == "fired")
param <- list(max_depth = 2, eta = 1, silent = 1, nthread = 2,
              objective = "binary:logistic", eval_metric = "auc")
HR_xgb_model <- xgb.train(param, data_train, nrounds = 50)
explainer_xgb <- explain(HR_xgb_model, data = model_martix_train,
                     y = HR$status == "fired", label = "xgboost")
vd_xgb <- feature_importance(explainer_xgb, type = "raw")
head(vd_xgb)
plot(vd_xgb)
 }
}
